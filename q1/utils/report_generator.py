"""
Report Generator for LLM Comparison Tool
Creates markdown reports from model comparison results
"""

import os
from datetime import datetime

class ReportGenerator:
    def __init__(self):
        self.report_template = """# LLM Model Comparison Report

**Generated on:** {timestamp}  
**Query:** "{query}"

## Results Summary

{results_table}

## Detailed Analysis

{detailed_analysis}

## Model Information

{model_info}

## Key Observations

{observations}

---
*Report generated by LLM Comparison Tool*
"""

    def generate_report(self, results, output_file):
        """Generate a markdown report from comparison results"""
        
        query = results['query']
        timestamp = results['timestamp']
        comparisons = results['comparisons']
        
        # Build the report content
        report_content = f"""# LLM Model Comparison Report

**Generated on:** {timestamp}  
**Query:** "{query}"

## Results Summary

{self._build_results_table(comparisons)}

## Detailed Analysis

{self._build_detailed_analysis(comparisons)}

## Key Observations

{self._build_observations(comparisons)}

---
*Report generated by LLM Comparison Tool*
"""
        
        # Write to file
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"Report generated: {output_file}")

    def _build_results_table(self, comparisons):
        """Build the main results comparison table"""
        table = "| Model Type | Model Name | Response Preview | Generation Time |\n"
        table += "|------------|------------|------------------|----------------|\n"
        
        for model_type, models in comparisons.items():
            for model_key, data in models.items():
                config = data['config']
                response_data = data['response_data']
                
                if isinstance(response_data, dict):
                    response_preview = response_data['response'][:80] + "..."
                    gen_time = f"{response_data['generation_time']}s"
                else:
                    response_preview = "Error occurred"
                    gen_time = "N/A"
                
                response_preview = response_preview.replace('\n', ' ').replace('|', '\\|')
                table += f"| {model_type.title()} | {model_key} | {response_preview} | {gen_time} |\n"
        
        return table

    def _build_detailed_analysis(self, comparisons):
        """Build detailed analysis section"""
        analysis = ""
        
        for model_type, models in comparisons.items():
            analysis += f"\n### {model_type.title()} Models\n\n"
            
            for model_key, data in models.items():
                config = data['config']
                response_data = data['response_data']
                
                analysis += f"#### {model_key}\n"
                analysis += f"**Description:** {config['description']}\n\n"
                
                if isinstance(response_data, dict):
                    analysis += f"**Full Response:**\n```\n{response_data['response']}\n```\n\n"
                    analysis += f"**Performance:** {response_data['generation_time']} seconds\n\n"
                else:
                    analysis += f"**Error:** {response_data}\n\n"
        
        return analysis

    def _build_model_info(self, comparisons):
        """Build model information section"""
        info = ""
        
        for model_type, models in comparisons.items():
            info += f"\n### {model_type.title()} Category\n\n"
            
            if model_type == 'base':
                info += "**Base models** are raw pretrained language models that excel at text completion but may not follow instructions well.\n\n"
            elif model_type == 'instruct':
                info += "**Instruct models** are fine-tuned to follow human instructions and engage in conversations.\n\n"
            elif model_type == 'fine-tuned':
                info += "**Fine-tuned models** are specialized for specific domains or tasks like code generation.\n\n"
            
            for model_key, data in models.items():
                config = data['config']
                info += f"- **{model_key}**: {config['model_name']} - {config['description']}\n"
        
        return info

    def _build_observations(self, comparisons):
        """Build key observations based on results"""
        observations = []
        
        model_types = list(comparisons.keys())
        
        if 'base' in comparisons:
            observations.append("Base models provide raw text completion")
        
        if 'instruct' in comparisons:
            observations.append("Instruct models are better at following instructions")
        
        if 'fine-tuned' in comparisons:
            observations.append("Fine-tuned models excel in specialized domains")
        
        observations.append("All models are free and run locally")
        
        # Performance observations
        fastest_model = None
        fastest_time = float('inf')
        
        for model_type, models in comparisons.items():
            for model_key, data in models.items():
                response_data = data['response_data']
                if isinstance(response_data, dict) and response_data['generation_time'] < fastest_time:
                    fastest_time = response_data['generation_time']
                    fastest_model = model_key
        
        if fastest_model:
            observations.append(f"**Performance:** {fastest_model} was the fastest, generating response in {fastest_time}s")
        
        # Model-specific observations
        if 'base' in comparisons:
            observations.append("**Base models:** May provide less structured responses as they're optimized for text completion rather than instruction following")
        
        if 'instruct' in comparisons:
            observations.append("**Instruct models:** Generally provide more coherent and instruction-aware responses")
        
        if 'fine-tuned' in comparisons:
            observations.append("**Fine-tuned models:** Show specialized knowledge in their domain but may be less versatile for general queries")
        
        # Add free model note
        observations.append("**Cost:** All models used are completely free and run locally using HuggingFace transformers")
        
        return "\n".join([f"- {obs}" for obs in observations])

    def generate_multiple_queries_report(self, all_results, output_file):
        """Generate a report for multiple queries (bonus feature)"""
        
        report = "# Multi-Query LLM Comparison Report\n\n"
        report += f"**Generated on:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"**Total Queries:** {len(all_results)}\n\n"
        
        # Summary table
        report += "## Summary Table\n\n"
        report += "| Query | Best Performing Model | Avg Generation Time |\n"
        report += "|-------|----------------------|--------------------|\n"
        
        for result in all_results:
            # Find best performing model (shortest time with valid response)
            best_model = "N/A"
            avg_time = 0
            valid_responses = 0
            
            for model_type, models in result['comparisons'].items():
                for model_key, data in models.items():
                    response_data = data['response_data']
                    if isinstance(response_data, dict):
                        avg_time += response_data['generation_time']
                        valid_responses += 1
                        if best_model == "N/A":
                            best_model = model_key
            
            avg_time = round(avg_time / valid_responses, 2) if valid_responses > 0 else 0
            
            query_preview = result['query'][:50] + "..." if len(result['query']) > 50 else result['query']
            report += f"| {query_preview} | {best_model} | {avg_time}s |\n"
        
        # Individual reports
        report += "\n## Individual Query Results\n\n"
        for i, result in enumerate(all_results, 1):
            report += f"### Query {i}: {result['query']}\n\n"
            report += self._build_results_table(result['comparisons'])
            report += "\n\n"
        
        # Write to file
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ðŸ“Š Multi-query report generated: {output_file}") 